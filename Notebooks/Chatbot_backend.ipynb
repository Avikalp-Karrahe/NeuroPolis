{"cells":[{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"Chatbot.ipynb\n","\n","Updated to read local document content (Corpus_Content.txt) for general Q&A,\n","and to include tweet validation, blockchain push functionality, and workarounds for NLTK tokenizer errors.\n","\"\"\"\n","\n","import io\n","import os\n","import re              # Needed for regular expression matching.\n","import random\n","import string        # For processing standard Python strings.\n","import warnings\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","warnings.filterwarnings('ignore')\n","\n","# Ensure NLTK uses the expected data directory.\n","os.environ[\"NLTK_DATA\"] = \"/root/nltk_data\"\n","\n","# Instead of using the Google Docs API, load document content from a local file.\n","# Change \"Corpus_Content.txt\" to your actual file name.\n","DOCUMENT_FILENAME = \"Corpus_Content.txt\"\n","if os.path.exists(DOCUMENT_FILENAME):\n","    with open(DOCUMENT_FILENAME, \"r\", encoding=\"utf-8\") as f:\n","        raw = f.read().lower()\n","    print(\"Local document content loaded from '{}'.\\n\".format(DOCUMENT_FILENAME))\n","else:\n","    raw = \"hello, this is a fallback text for the chatbot. feel free to ask me anything about our system.\"\n","    print(\"Using fallback text.\\n\")\n","\n","# Install nltk if not already installed.\n","!pip install -q nltk\n","\n","import nltk\n","# Download required NLTK resources.\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","from nltk.stem import WordNetLemmatizer\n","\n","# Attempt to load the English Punkt tokenizer.\n","try:\n","    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n","except LookupError as e:\n","    print(\"Error loading tokenizer:\", e)\n","    print(\"Attempting to download 'punkt_tab' as suggested by the error message...\")\n","    nltk.download('punkt_tab')\n","    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n","\n","sent_tokens = tokenizer.tokenize(raw)  # List of sentences.\n","word_tokens = nltk.word_tokenize(raw)    # List of words.\n","\n","lemmer = WordNetLemmatizer()\n","def LemTokens(tokens):\n","    return [lemmer.lemmatize(token) for token in tokens]\n","remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n","def LemNormalize(text):\n","    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n","\n","GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\",)\n","GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n","def greeting(sentence):\n","    for word in sentence.split():\n","        if word.lower() in GREETING_INPUTS:\n","            return random.choice(GREETING_RESPONSES)\n","\n","def response(user_response):\n","    robo_response = ''\n","    sent_tokens.append(user_response)\n","    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n","    tfidf = TfidfVec.fit_transform(sent_tokens)\n","    vals = cosine_similarity(tfidf[-1], tfidf)\n","    idx = vals.argsort()[0][-2]\n","    flat = vals.flatten()\n","    flat.sort()\n","    req_tfidf = flat[-2]\n","    if req_tfidf == 0:\n","        robo_response += \"I am sorry! I don't understand you\"\n","        return robo_response\n","    else:\n","        robo_response += sent_tokens[idx]\n","        return robo_response\n","\n","##############################\n","# Tweet Validation Integration\n","##############################\n","\n","import requests\n","import datetime as dt\n","import urllib3\n","urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n","\n","# Set your blockchain ledger endpoint – update if needed.\n","LEDGER_RPC = \"https://4c7f-209-129-88-187.ngrok-free.app\"\n","SUBMIT_URL  = f\"{LEDGER_RPC}/submit\"\n","\n","# Load sensor data (ensure \"final_sensors_zones.csv\" is in your working directory).\n","import pandas as pd\n","sensors = pd.read_csv(\"/final_sensors_zones.csv\")\n","sensors[\"timestamp\"] = pd.to_datetime(sensors[\"timestamp\"])\n","\n","# Hazard keyword patterns and helper functions from your tweet classification notebook.\n","HAZARD_PATTERNS = {\n","    \"fire\":       r\"\\bfire\\b\",\n","    \"earthquake\": r\"\\bearthquake\\b|\\bquake\\b\",\n","    \"flood\":      r\"\\bflood\\b\",\n","    \"evacuate\":   r\"\\bevacuate\\b|\\bevacuation\\b|\\bemergency\\b\"\n","}\n","\n","def extract_hazard(text):\n","    \"\"\"Extract a hazard keyword from tweet text.\"\"\"\n","    for hazard, pattern in HAZARD_PATTERNS.items():\n","        if re.search(pattern, str(text), flags=re.I):\n","            return hazard\n","    return \"unknown\"\n","\n","def map_hazard_to_sensors(hazard):\n","    \"\"\"Map hazard types to relevant sensor types.\"\"\"\n","    mapping = {\n","        \"flood\":      [\"flood\", \"humidity\"],\n","        \"fire\":       [\"temp\", \"humidity\"],\n","        \"earthquake\": [\"seismic\"],\n","        \"evacuate\":   [\"flood\", \"temp\", \"seismic\", \"humidity\"],\n","    }\n","    return mapping.get(hazard, [])\n","\n","VALID_THRESHOLD = 60   # Sensor threshold (1-100 scale)\n","TIME_WINDOW = \"20min\"  # ± window for sensor data\n","MIN_ACTIVE = 1\n","HIGH_PCT_REQ = 20      # Percent threshold to consider hazard real\n","MEAN_REQ = 20\n","\n","def validate_tweet(row):\n","    \"\"\"\n","    Validate a tweet based on sensor readings.\n","    Expects row with keys: tweet_hazard, zone_id, and timestamp.\n","    Returns a tuple: (verdict, pct_high, mean_read, n_active)\n","    \"\"\"\n","    if row[\"tweet_hazard\"] == \"unknown\":\n","        return \"ignore\", 0, 0, 0\n","\n","    zone = row[\"zone_id\"]\n","    hazard = row[\"tweet_hazard\"]\n","    tweet_time = row[\"timestamp\"]\n","\n","    window = sensors[\n","        (sensors[\"zone_id\"] == zone) &\n","        (sensors[\"sensor_type\"].isin(map_hazard_to_sensors(hazard))) &\n","        (abs(sensors[\"timestamp\"] - tweet_time) <= pd.Timedelta(TIME_WINDOW))\n","    ]\n","\n","    if window.empty:\n","        return \"fake\", 0, 0, 0\n","\n","    mean_read = window[\"reading_value\"].mean()\n","    pct_high = (window[\"reading_value\"] >= VALID_THRESHOLD).mean() * 100\n","    n_active = (window[\"status\"].str.lower() == \"active\").sum()\n","\n","    if pct_high >= HIGH_PCT_REQ and mean_read >= MEAN_REQ and n_active >= MIN_ACTIVE:\n","        verdict = \"valid\"\n","    elif pct_high < 5:\n","        verdict = \"fake\"\n","    else:\n","        verdict = \"uncertain\"\n","    return verdict, pct_high, mean_read, n_active\n","\n","def push_tweet(payload):\n","    \"\"\"\n","    Push tweet payload to the blockchain ledger.\n","    Returns the HTTP status code.\n","    \"\"\"\n","    response = requests.post(SUBMIT_URL, json=payload, timeout=10, verify=False)\n","    return response.status_code\n","\n","def process_chatbot_tweet(tweet_text, zone_id, ts_str):\n","    \"\"\"\n","    Process a tweet from the chatbot:\n","      1. Convert timestamp string to a datetime object.\n","      2. Extract hazard from tweet text.\n","      3. Validate tweet using sensor data.\n","      4. Build payload and push it to the blockchain.\n","    Returns a response string.\n","    \"\"\"\n","    try:\n","        tweet_timestamp = pd.to_datetime(ts_str)\n","    except Exception as e:\n","        return f\"Invalid timestamp format: {e}\"\n","\n","    tweet_hazard = extract_hazard(tweet_text)\n","    tweet_dict = {\n","        \"text\": tweet_text,\n","        \"zone_id\": zone_id,\n","        \"timestamp\": tweet_timestamp,\n","        \"tweet_hazard\": tweet_hazard\n","    }\n","    verdict, pct_high, mean_read, n_active = validate_tweet(tweet_dict)\n","\n","    payload = {\n","        \"tweet_id\": \"manual-\" + dt.datetime.now().strftime(\"%Y%m%d%H%M%S\"),\n","        \"zone_id\": zone_id,\n","        \"hazard\": tweet_hazard,\n","        \"status\": verdict,\n","        \"timestamp\": tweet_timestamp.isoformat(),\n","        \"pct_high\": pct_high,\n","        \"mean_read\": mean_read,\n","        \"n_active\": n_active\n","    }\n","    push_result = push_tweet(payload)\n","    if push_result == 200:\n","        return (f\"Tweet validated as '{verdict}'. Mean sensor reading: {mean_read:.2f}, \"\n","                f\"percent high: {pct_high:.1f}%, active sensors: {n_active}. \"\n","                f\"Successfully pushed to blockchain.\")\n","    else:\n","        return f\"Error sending tweet to the ledger. Status code: {push_result}\"\n","\n","##############################\n","# Chatbot Main Loop\n","##############################\n","flag = True\n","print(\"My name is Freestyle Genie. I will answer your queries and validate tweets. To exit, type 'cola'!\")\n","while flag:\n","    user_response = input()\n","    user_response = user_response.lower()\n","\n","    # If the user enters \"validate tweet\", enter tweet validation mode.\n","    if \"validate tweet\" in user_response:\n","        tweet_text = input(\"Enter tweet text: \")\n","        zone_id = input(\"Enter zone (e.g., ZoneA): \")\n","        ts_str = input(\"Enter tweet timestamp (YYYY-MM-DD HH:MM:SS): \")\n","        result = process_chatbot_tweet(tweet_text, zone_id, ts_str)\n","        print(\"Freestyle Genie:\", result)\n","    elif user_response != 'cola':\n","        if user_response in ['thanks', 'thank you']:\n","            flag = False\n","            print(\"You are welcome..\")\n","        else:\n","            if greeting(user_response) is not None:\n","                print(\"Freestyle Genie:\" + greeting(user_response))\n","            else:\n","                print(\"Freestyle Genie:\", end=\" \")\n","                print(response(user_response))\n","                sent_tokens.remove(user_response)\n","    else:\n","        flag = False\n","        print(\"Freestyle Genie: Bye! take care..\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LKtnf5r7spbb","executionInfo":{"status":"ok","timestamp":1744710340581,"user_tz":420,"elapsed":276785,"user":{"displayName":"Avikalp Karrahe","userId":"14204401851323774624"}},"outputId":"321e3d53-a84d-4d3a-be88-64a12fdf45c4"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Local document content loaded from 'Corpus_Content.txt'.\n","\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["My name is Freestyle Genie. I will answer your queries and validate tweets. To exit, type 'cola'!\n","zone 1 summary\n","Freestyle Genie: zone 0.0 summary:\n","total events: 1105.\n","most common risk level: low.\n","how many flood in zone 2\n","Freestyle Genie: most common disaster type: flood.\n","zone 5 most common disater\n","Freestyle Genie: zone 0.0 summary:\n","total events: 1105.\n","most common risk level: low.\n","what is this dashboard\n","Freestyle Genie: * explanatory note:\n"," this dashboard is part of a broader crisis management project designed for smart city applications.\n","exit\n","Freestyle Genie: I am sorry! I don't understand you\n","cola\n","Freestyle Genie: Bye! take care..\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T7D11rCwpOS_","executionInfo":{"status":"ok","timestamp":1744707156369,"user_tz":420,"elapsed":633,"user":{"displayName":"Avikalp Karrahe","userId":"14204401851323774624"}},"outputId":"dd2fd065-d58e-4132-e351-9bd033a9a0ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}